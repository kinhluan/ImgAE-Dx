{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImgAE-Dx Enhanced Training with Advanced Features (Fixed)\n",
    "\n",
    "This notebook implements research-grade training for medical image anomaly detection with:\n",
    "- Validation split and monitoring\n",
    "- Early stopping mechanism\n",
    "- Learning rate scheduling (Cosine Annealing)\n",
    "- Gradient clipping\n",
    "- Warmup epochs\n",
    "- **FIXED:** Advanced checkpoint management with proper save_frequency\n",
    "\n",
    "## Quick Start:\n",
    "1. Set `CONFIG['model_type'] = 'both'` to train both U-Net and Reversed AE\n",
    "2. Set `CONFIG['keep_best_only'] = False` to enable regular checkpoints every N epochs\n",
    "3. Run cells in order\n",
    "4. Monitor training progress with validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and mount Google Drive\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "    print(f\"GPU: {gpu_info[0]}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Please enable GPU in Runtime > Change runtime type\")\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install ImgAE-Dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if not exists\n",
    "if not os.path.exists('/content/ImgAE-Dx'):\n",
    "    !git clone https://github.com/kinhluan/ImgAE-Dx.git /content/ImgAE-Dx\n",
    "    %cd /content/ImgAE-Dx\n",
    "else:\n",
    "    %cd /content/ImgAE-Dx\n",
    "    !git pull\n",
    "\n",
    "# Add the src directory to Python path\n",
    "import sys\n",
    "if '/content/ImgAE-Dx/src' not in sys.path:\n",
    "    sys.path.append('/content/ImgAE-Dx/src')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -e .\n",
    "!pip install datasets transformers accelerate\n",
    "!pip install wandb --upgrade\n",
    "\n",
    "print(\"‚úÖ ImgAE-Dx installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Configuration (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OPTIMIZED CONFIG - Ph√π h·ª£p v·ªõi dataset hf-vision/chest-xray-pneumonia\nCONFIG = {\n    # Model settings\n    'model_type': 'both',  # 'unet', 'reversed_ae', or 'both'\n    \n    # Dataset source\n    'dataset_source': 'huggingface',\n    \n    # HuggingFace Dataset settings\n    'hf_dataset': 'hf-vision/chest-xray-pneumonia',\n    'hf_config': None,\n    'hf_split': 'train',\n    'hf_streaming': False,\n    'hf_token': None,\n    'image_column': 'image',\n    'label_column': 'labels',\n    \n    # Training settings - OPTIMIZED cho dataset nh·ªè (1,341 NORMAL images)\n    'samples': 1200,        # 90% c·ªßa NORMAL images available (1,341 total)\n    'epochs': 250,          # TƒÉng epochs v√¨ √≠t data\n    'batch_size': 16,       # Gi·∫£m batch size cho dataset nh·ªè\n    'learning_rate': 5e-6,  # Gi·∫£m LR cho stable training\n    'image_size': 128,\n    \n    # Advanced training features - TUNED\n    'validation_split': 0.25,         # TƒÉng validation v√¨ val set g·ªëc ch·ªâ 8 images\n    'early_stopping_patience': 50,    # TƒÉng patience v√¨ c·∫ßn train l√¢u\n    'lr_scheduler': 'cosine',         # Better convergence\n    'gradient_clip_norm': 0.5,        # Gi·∫£m cho dataset nh·ªè\n    'warmup_epochs': 10,              # TƒÉng warmup cho stability\n    'min_lr_factor': 0.0001,          # LR cu·ªëi r·∫•t nh·ªè\n    \n    # T4 optimizations - TUNED cho small dataset\n    'mixed_precision': True,\n    'memory_limit': 13,                  # Leave 3GB headroom\n    'gradient_accumulation_steps': 4,    # Effective batch = 16*4 = 64\n    'num_workers': 2,                    # Reduced for memory\n    \n    # Enhanced checkpointing\n    'checkpoint_dir': '/content/drive/MyDrive/imgae_dx_enhanced_checkpoints',\n    'save_frequency': 10,             # Save m·ªói 10 epochs\n    'keep_best_only': False,          # L∆∞u c·∫£ regular checkpoints\n    'resume_from_checkpoint': None,   # Path to resume from\n    'resume_model_type': None,        # Which model to resume\n    \n    # Logging\n    'use_wandb': False,\n    'wandb_project': 'imgae-dx-enhanced',\n    'wandb_run_name': None,\n    'log_frequency': 5,               # Log m·ªói 5 batches (v√¨ √≠t batches)\n}\n\n# Create directories\nos.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\nos.makedirs('/content/outputs/logs', exist_ok=True)\nos.makedirs('/content/outputs/plots', exist_ok=True)\n\n# Validation checks\nprint(\"üîç DATASET & CONFIG VALIDATION:\")\nprint(f\"   Dataset: {CONFIG['hf_dataset']}\")\nprint(f\"   Max NORMAL images available: 1,341\")\nprint(f\"   Requested samples: {CONFIG['samples']} {'‚úÖ' if CONFIG['samples'] <= 1341 else '‚ùå TOO MANY!'}\")\nprint(f\"   Train samples: {int(CONFIG['samples'] * (1 - CONFIG['validation_split']))}\")\nprint(f\"   Val samples: {int(CONFIG['samples'] * CONFIG['validation_split'])}\")\n\nprint(f\"\\nüöÄ OPTIMIZED CONFIG FOR SMALL DATASET:\")\nprint(f\"   Model: {CONFIG['model_type']}\")\nprint(f\"   Samples: {CONFIG['samples']} ({CONFIG['validation_split']:.1%} validation)\")\nprint(f\"   Epochs: {CONFIG['epochs']} (patience: {CONFIG['early_stopping_patience']})\")\nprint(f\"   Batch: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation_steps']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']} effective\")\nprint(f\"   LR: {CONFIG['learning_rate']:.0e} ‚Üí {CONFIG['learning_rate'] * CONFIG['min_lr_factor']:.0e}\")\nprint(f\"   Checkpoints: Every {CONFIG['save_frequency']} epochs + best model\")\n\n# Estimated training time\nbatches_per_epoch = int(CONFIG['samples'] * (1 - CONFIG['validation_split'])) // CONFIG['batch_size']\nprint(f\"\\n‚è±Ô∏è TRAINING ESTIMATES:\")\nprint(f\"   Batches per epoch: {batches_per_epoch}\")\nprint(f\"   Est. time per model: ~{CONFIG['epochs'] * batches_per_epoch * 2 / 3600:.1f} hours\")\nprint(f\"   Total est. time (both models): ~{CONFIG['epochs'] * batches_per_epoch * 4 / 3600:.1f} hours\")\n\nprint(f\"\\nüí° OPTIMIZATION NOTES:\")\nprint(f\"   - Small dataset requires more epochs and lower LR\")\nprint(f\"   - High validation split (25%) due to tiny original val set\")\nprint(f\"   - Gradient accumulation compensates for small batch size\")\nprint(f\"   - Early stopping patience increased for convergence\")\nprint(f\"   - Target: Train loss < 0.05, Val loss < 0.08 for good anomaly detection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Weights & Biases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['use_wandb']:\n",
    "    import wandb\n",
    "    \n",
    "    wandb.login()\n",
    "    \n",
    "    run_name = CONFIG['wandb_run_name'] or f\"enhanced_{CONFIG['model_type']}_{CONFIG['samples']}samples\"\n",
    "    wandb.init(\n",
    "        project=CONFIG['wandb_project'],\n",
    "        name=run_name,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    print(f\"‚úÖ W&B initialized: {run_name}\")\n",
    "else:\n",
    "    print(\"W&B logging disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Dataset Loading with Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(f\"üìÇ Loading dataset: {CONFIG['hf_dataset']}...\")\n",
    "\n",
    "try:\n",
    "    # Authentication if needed\n",
    "    auth_kwargs = {'use_auth_token': CONFIG['hf_token']} if CONFIG['hf_token'] else {}\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\n",
    "        CONFIG['hf_dataset'],\n",
    "        CONFIG['hf_config'],\n",
    "        split=CONFIG['hf_split'],\n",
    "        **auth_kwargs\n",
    "    )\n",
    "    \n",
    "    # Filter for NORMAL images only (label = 0)\n",
    "    # For unsupervised anomaly detection, we only train on normal images\n",
    "    normal_dataset = dataset.filter(lambda x: x[CONFIG['label_column']] == 0)\n",
    "    print(f\"üìä Filtered to {len(normal_dataset)} NORMAL images from {len(dataset)} total\")\n",
    "    \n",
    "    # Take specified number of samples\n",
    "    if len(normal_dataset) > CONFIG['samples']:\n",
    "        normal_dataset = normal_dataset.select(range(CONFIG['samples']))\n",
    "    \n",
    "    print(f\"‚úÖ Using {len(normal_dataset)} NORMAL images for training\")\n",
    "    print(f\"Dataset features: {normal_dataset.features}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Enhanced dataset wrapper\n",
    "class HFImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Get image\n",
    "        image = None\n",
    "        for col in [CONFIG['image_column'], 'image', 'img', 'pixel_values']:\n",
    "            if col in item:\n",
    "                image = item[col]\n",
    "                break\n",
    "        \n",
    "        if image is None:\n",
    "            raise ValueError(f\"No image column found. Available: {list(item.keys())}\")\n",
    "        \n",
    "        # Convert to PIL if needed\n",
    "        if not isinstance(image, Image.Image):\n",
    "            if isinstance(image, np.ndarray):\n",
    "                image = Image.fromarray(image)\n",
    "            else:\n",
    "                image = Image.fromarray(np.array(image))\n",
    "        \n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        return self.transform(image)\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = HFImageDataset(normal_dataset, transform)\n",
    "\n",
    "# Split into train and validation\n",
    "val_size = int(CONFIG['validation_split'] * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Reproducible split\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset split:\")\n",
    "print(f\"   Training: {len(train_dataset)} images\")\n",
    "print(f\"   Validation: {len(val_dataset)} images\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgae_dx.models import UNet, ReversedAutoencoder\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model initialization\n",
    "models_to_train = []\n",
    "\n",
    "if CONFIG['model_type'] in ['unet', 'both']:\n",
    "    unet = UNet(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        features=[64, 128, 256, 512]\n",
    "    ).to(device)\n",
    "    models_to_train.append(('unet', unet))\n",
    "    print(\"‚úÖ U-Net initialized\")\n",
    "\n",
    "if CONFIG['model_type'] in ['reversed_ae', 'both']:\n",
    "    reversed_ae = ReversedAutoencoder(\n",
    "        in_channels=1,\n",
    "        latent_dim=128,\n",
    "        image_size=CONFIG['image_size']\n",
    "    ).to(device)\n",
    "    models_to_train.append(('reversed_ae', reversed_ae))\n",
    "    print(\"‚úÖ Reversed Autoencoder initialized\")\n",
    "\n",
    "# Count parameters\n",
    "print(\"\\nüìä Model Parameters:\")\n",
    "for name, model in models_to_train:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   {name}: {trainable_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enhanced Training with FIXED Checkpoint Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport math\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch.nn.utils as torch_utils\n\n# T4 optimizations\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.cuda.set_per_process_memory_fraction(CONFIG['memory_limit'] / 16.0)\n    print(f\"‚úÖ T4 optimizations enabled (Memory: {CONFIG['memory_limit']}GB)\")\n\nclass EarlyStopping:\n    \"\"\"Early stopping utility class\"\"\"\n    def __init__(self, patience=8, min_delta=0.0001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        return self.early_stop\n\ndef cosine_warmup_scheduler(optimizer, warmup_epochs, total_epochs, min_lr_factor=0.1):\n    \"\"\"Create cosine annealing scheduler with warmup\"\"\"\n    initial_lr = optimizer.param_groups[0]['lr']\n    min_lr = initial_lr * min_lr_factor\n    \n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            # Warmup phase: linear increase from min_lr to initial_lr\n            return min_lr_factor + (1 - min_lr_factor) * epoch / warmup_epochs\n        else:\n            # Cosine annealing phase\n            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n            return min_lr_factor + (1 - min_lr_factor) * 0.5 * (1 + math.cos(math.pi * progress))\n    \n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\ndef validate_model(model, val_loader, criterion, device):\n    \"\"\"Validate model on validation set\"\"\"\n    model.eval()\n    val_loss = 0.0\n    val_batches = 0\n    \n    with torch.no_grad():\n        for images in val_loader:\n            images = images.to(device)\n            \n            if CONFIG['mixed_precision']:\n                with autocast():\n                    reconstructed = model(images)\n                    loss = criterion(reconstructed, images)\n            else:\n                reconstructed = model(images)\n                loss = criterion(reconstructed, images)\n            \n            val_loss += loss.item()\n            val_batches += 1\n    \n    return val_loss / val_batches if val_batches > 0 else float('inf')\n\ndef enhanced_train_model(model_name, model, train_loader, val_loader, config):\n    \"\"\"Enhanced training function with FIXED checkpoint logic and gradient accumulation\"\"\"\n    print(f\"\\nüöÄ Enhanced Training: {model_name}\")\n    print(f\"   Features: Validation monitoring, Early stopping, LR scheduling, Gradient clipping\")\n    print(f\"   Gradient Accumulation: {config['gradient_accumulation_steps']} steps\")\n    print(f\"   Checkpointing: Every {config['save_frequency']} epochs {'+ best model' if not config['keep_best_only'] else '(best only)'}\")\n    \n    # Setup optimizer and loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.MSELoss()\n    \n    # Mixed precision setup\n    scaler = GradScaler() if config['mixed_precision'] else None\n    \n    # Learning rate scheduler\n    if config['lr_scheduler'] == 'cosine':\n        scheduler = cosine_warmup_scheduler(\n            optimizer, \n            config['warmup_epochs'], \n            config['epochs'],\n            config['min_lr_factor']\n        )\n    else:\n        scheduler = None\n    \n    # Early stopping\n    early_stopping = EarlyStopping(patience=config['early_stopping_patience'])\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'learning_rate': []\n    }\n    \n    # Resume from checkpoint if specified\n    start_epoch = 0\n    best_val_loss = float('inf')\n    \n    if (config.get('resume_from_checkpoint') and \n        config.get('resume_model_type') == model_name):\n        \n        print(f\"üìÇ Resuming {model_name} from checkpoint...\")\n        try:\n            checkpoint = torch.load(config['resume_from_checkpoint'], map_location=device)\n            \n            if 'model_state_dict' in checkpoint:\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_val_loss = checkpoint.get('val_loss', float('inf'))\n                if 'history' in checkpoint:\n                    history = checkpoint['history']\n                print(f\"‚úÖ Resumed from epoch {start_epoch}, best val loss: {best_val_loss:.4f}\")\n            else:\n                model.load_state_dict(checkpoint)\n                print(f\"‚úÖ Loaded model weights, starting fresh training\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not resume: {e}. Starting fresh...\")\n    \n    # Training loop with gradient accumulation\n    accumulation_steps = config['gradient_accumulation_steps']\n    \n    for epoch in range(start_epoch, config['epochs']):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_batches = 0\n        accumulated_batches = 0\n        \n        # Get current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [LR: {current_lr:.2e}]\")\n        \n        # Zero gradients at the start\n        optimizer.zero_grad()\n        \n        for batch_idx, images in enumerate(pbar):\n            images = images.to(device)\n            \n            # Forward pass with mixed precision\n            if config['mixed_precision']:\n                with autocast():\n                    reconstructed = model(images)\n                    loss = criterion(reconstructed, images)\n                    # Scale loss by accumulation steps\n                    loss = loss / accumulation_steps\n                \n                # Backward pass\n                scaler.scale(loss).backward()\n                \n                # Update weights every accumulation_steps batches\n                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    if config.get('gradient_clip_norm'):\n                        scaler.unscale_(optimizer)\n                        torch_utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])\n                    \n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    accumulated_batches += 1\n            else:\n                reconstructed = model(images)\n                loss = criterion(reconstructed, images)\n                # Scale loss by accumulation steps\n                loss = loss / accumulation_steps\n                \n                loss.backward()\n                \n                # Update weights every accumulation_steps batches\n                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    if config.get('gradient_clip_norm'):\n                        torch_utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])\n                    \n                    optimizer.step()\n                    optimizer.zero_grad()\n                    accumulated_batches += 1\n            \n            # Update metrics (use unscaled loss for tracking)\n            train_loss += loss.item() * accumulation_steps\n            train_batches += 1\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f'{loss.item() * accumulation_steps:.4f}',\n                'avg_loss': f'{train_loss/train_batches:.4f}',\n                'accum': f'{(batch_idx % accumulation_steps) + 1}/{accumulation_steps}'\n            })\n            \n            # Log to W&B\n            if config['use_wandb'] and batch_idx % config.get('log_frequency', 10) == 0:\n                wandb.log({\n                    f'{model_name}_batch_loss': loss.item() * accumulation_steps,\n                    f'{model_name}_learning_rate': current_lr,\n                    'epoch': epoch,\n                    'batch': batch_idx\n                })\n        \n        # Calculate epoch metrics\n        avg_train_loss = train_loss / train_batches\n        \n        # Validation phase\n        avg_val_loss = validate_model(model, val_loader, criterion, device)\n        \n        # Update history\n        history['train_loss'].append(avg_train_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['learning_rate'].append(current_lr)\n        \n        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, LR = {current_lr:.2e}\")\n        \n        # Log epoch metrics\n        if config['use_wandb']:\n            wandb.log({\n                f'{model_name}_train_loss': avg_train_loss,\n                f'{model_name}_val_loss': avg_val_loss,\n                f'{model_name}_learning_rate': current_lr,\n                'epoch': epoch + 1\n            })\n        \n        # Update learning rate\n        if scheduler:\n            scheduler.step()\n        \n        # FIXED: Checkpoint saving logic\n        is_best_model = False\n        is_regular_save = (epoch + 1) % config['save_frequency'] == 0\n        \n        # Check if this is the best model so far\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            is_best_model = True\n            \n            # Always save best model\n            best_path = f\"{config['checkpoint_dir']}/{model_name}_best.pth\"\n            torch.save(model.state_dict(), best_path)\n            print(f\"‚úÖ Best model saved: {best_path} (Val Loss: {best_val_loss:.4f})\")\n        \n        # Save regular checkpoint based on save_frequency\n        if is_regular_save:\n            if config.get('keep_best_only', False):\n                print(f\"‚è≠Ô∏è Epoch {epoch+1}: Regular checkpoint skipped (keep_best_only=True)\")\n            else:\n                checkpoint_path = f\"{config['checkpoint_dir']}/{model_name}_epoch_{epoch+1}.pth\"\n                torch.save({\n                    'epoch': epoch + 1,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'train_loss': avg_train_loss,\n                    'val_loss': avg_val_loss,\n                    'history': history,\n                    'config': config\n                }, checkpoint_path)\n                print(f\"üíæ Regular checkpoint saved: {checkpoint_path}\")\n        \n        # Early stopping check\n        if early_stopping(avg_val_loss):\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    return model, history\n\n# Train models\nall_histories = {}\ntrained_models = {}\n\nprint(f\"üöÄ Starting enhanced training for {len(models_to_train)} model(s)...\")\nprint(f\"üíæ Checkpoint strategy: {'Best model only' if CONFIG['keep_best_only'] else f'Every {CONFIG[\"save_frequency\"]} epochs + best model'}\")\n\nfor model_name, model in models_to_train:\n    start_time = time.time()\n    \n    # Enhanced training\n    trained_model, history = enhanced_train_model(\n        model_name, model, train_loader, val_loader, CONFIG\n    )\n    \n    # Store results\n    trained_models[model_name] = trained_model\n    all_histories[model_name] = history\n    \n    # Calculate training time\n    training_time = (time.time() - start_time) / 60\n    print(f\"\\n‚úÖ {model_name} enhanced training completed in {training_time:.1f} minutes\")\n    \n    # Clear cache between models\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(f\"\\nüéâ All models trained successfully with FIXED checkpoint logic and gradient accumulation!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enhanced Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced training visualization with parameter logging\nfig, axes = plt.subplots(3, 3, figsize=(20, 15))\n\n# Training info text for title\ntraining_info = (f\"Dataset: {CONFIG['hf_dataset'].split('/')[-1]} | \"\n                f\"Samples: {CONFIG['samples']} | \"\n                f\"Batch: {CONFIG['batch_size']}x{CONFIG['gradient_accumulation_steps']} | \"\n                f\"LR: {CONFIG['learning_rate']:.0e}\")\n\n# Plot 1: Training and Validation Loss\nfor model_name, history in all_histories.items():\n    epochs = range(1, len(history['train_loss']) + 1)\n    axes[0, 0].plot(epochs, history['train_loss'], label=f'{model_name} train', linestyle='-', linewidth=2)\n    axes[0, 0].plot(epochs, history['val_loss'], label=f'{model_name} val', linestyle='--', linewidth=2)\n\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss (MSE)')\naxes[0, 0].set_title('Training and Validation Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_yscale('log')  # Log scale for better visualization\n\n# Plot 2: Learning Rate Schedule\nfor model_name, history in all_histories.items():\n    epochs = range(1, len(history['learning_rate']) + 1)\n    axes[0, 1].plot(epochs, history['learning_rate'], label=f'{model_name} LR', linewidth=2)\n\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Learning Rate')\naxes[0, 1].set_title('Learning Rate Schedule')\naxes[0, 1].set_yscale('log')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Loss Improvement Rate (derivative)\nfor model_name, history in all_histories.items():\n    train_losses = history['train_loss']\n    val_losses = history['val_loss']\n    \n    # Calculate improvement rate (negative derivative)\n    train_improvement = [-1 * (train_losses[i] - train_losses[i-1]) for i in range(1, len(train_losses))]\n    val_improvement = [-1 * (val_losses[i] - val_losses[i-1]) for i in range(1, len(val_losses))]\n    \n    epochs = range(2, len(train_losses) + 1)\n    axes[0, 2].plot(epochs, train_improvement, label=f'{model_name} train improve', linestyle='-', alpha=0.7)\n    axes[0, 2].plot(epochs, val_improvement, label=f'{model_name} val improve', linestyle='--', alpha=0.7)\n\naxes[0, 2].set_xlabel('Epoch')\naxes[0, 2].set_ylabel('Loss Improvement Rate')\naxes[0, 2].set_title('Loss Improvement per Epoch')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\naxes[0, 2].axhline(y=0, color='red', linestyle=':', alpha=0.5)\n\n# Plot 4: Best Validation Loss Progress\nfor model_name, history in all_histories.items():\n    val_losses = history['val_loss']\n    best_val_loss = [min(val_losses[:i+1]) for i in range(len(val_losses))]\n    epochs = range(1, len(best_val_loss) + 1)\n    axes[1, 0].plot(epochs, best_val_loss, label=f'{model_name} best val', linewidth=2)\n\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Best Validation Loss')\naxes[1, 0].set_title('Best Validation Loss Progress')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].set_yscale('log')\n\n# Plot 5: Train vs Val Loss Correlation\ncolors = ['blue', 'red', 'green', 'purple']\nfor i, (model_name, history) in enumerate(all_histories.items()):\n    axes[1, 1].scatter(history['train_loss'], history['val_loss'], \n                      label=f'{model_name}', alpha=0.7, s=30, c=colors[i % len(colors)])\n\n# Add diagonal line\nmin_loss = min([min(h['train_loss'] + h['val_loss']) for h in all_histories.values()])\nmax_loss = max([max(h['train_loss'] + h['val_loss']) for h in all_histories.values()])\naxes[1, 1].plot([min_loss, max_loss], [min_loss, max_loss], 'k--', alpha=0.5, label='Perfect correlation')\n\naxes[1, 1].set_xlabel('Training Loss')\naxes[1, 1].set_ylabel('Validation Loss')\naxes[1, 1].set_title('Training vs Validation Loss Correlation')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].set_xscale('log')\naxes[1, 1].set_yscale('log')\n\n# Plot 6: Training Summary Statistics\ntraining_stats = []\nfor model_name, history in all_histories.items():\n    stats = {\n        'Model': model_name,\n        'Final Train Loss': f\"{history['train_loss'][-1]:.4f}\",\n        'Final Val Loss': f\"{history['val_loss'][-1]:.4f}\",\n        'Best Val Loss': f\"{min(history['val_loss']):.4f}\",\n        'Best Val Epoch': f\"{history['val_loss'].index(min(history['val_loss'])) + 1}\",\n        'Total Epochs': f\"{len(history['train_loss'])}\",\n        'Final LR': f\"{history['learning_rate'][-1]:.2e}\",\n        'LR Reduction': f\"{history['learning_rate'][0] / history['learning_rate'][-1]:.1f}x\",\n        'Convergence': '‚úÖ' if history['val_loss'][-1] < 0.08 else '‚ö†Ô∏è',\n        'Overfitting': '‚ö†Ô∏è' if history['train_loss'][-1] < history['val_loss'][-1] * 0.7 else '‚úÖ'\n    }\n    training_stats.append(stats)\n\n# Create text summary\naxes[1, 2].axis('off')\nsummary_text = \"TRAINING SUMMARY\\n\" + \"=\"*50 + \"\\n\\n\"\n\nfor stats in training_stats:\n    summary_text += f\"ü§ñ {stats['Model'].upper()}:\\n\"\n    summary_text += f\"   Final Train Loss: {stats['Final Train Loss']}\\n\"\n    summary_text += f\"   Final Val Loss: {stats['Final Val Loss']}\\n\"\n    summary_text += f\"   Best Val Loss: {stats['Best Val Loss']} (Epoch {stats['Best Val Epoch']})\\n\"\n    summary_text += f\"   Epochs: {stats['Total Epochs']}\\n\"\n    summary_text += f\"   LR: {stats['Final LR']} (reduced {stats['LR Reduction']})\\n\"\n    summary_text += f\"   Convergence: {stats['Convergence']} | Overfitting: {stats['Overfitting']}\\n\\n\"\n\n# Add config summary\nsummary_text += f\"üìä CONFIGURATION:\\n\"\nsummary_text += f\"   Dataset: {CONFIG['hf_dataset']}\\n\"\nsummary_text += f\"   Samples: {CONFIG['samples']} ({CONFIG['validation_split']:.0%} val split)\\n\"\nsummary_text += f\"   Batch Size: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation_steps']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\\n\"\nsummary_text += f\"   Learning Rate: {CONFIG['learning_rate']:.0e}\\n\"\nsummary_text += f\"   Scheduler: {CONFIG['lr_scheduler']}\\n\"\nsummary_text += f\"   Early Stopping: {CONFIG['early_stopping_patience']} epochs\\n\"\nsummary_text += f\"   Gradient Clip: {CONFIG['gradient_clip_norm']}\\n\"\n\naxes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, \n               fontsize=9, verticalalignment='top', fontfamily='monospace',\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n\n# Plot 7: Loss Smoothed (Moving Average)\nwindow_size = max(5, len(list(all_histories.values())[0]['train_loss']) // 20)\nfor model_name, history in all_histories.items():\n    train_losses = history['train_loss']\n    val_losses = history['val_loss']\n    \n    # Calculate moving average\n    def moving_average(data, window):\n        return [sum(data[i:i+window])/window for i in range(len(data)-window+1)]\n    \n    if len(train_losses) >= window_size:\n        train_smooth = moving_average(train_losses, window_size)\n        val_smooth = moving_average(val_losses, window_size)\n        epochs = range(window_size, len(train_losses) + 1)\n        \n        axes[2, 0].plot(epochs, train_smooth, label=f'{model_name} train (MA{window_size})', linestyle='-', alpha=0.8)\n        axes[2, 0].plot(epochs, val_smooth, label=f'{model_name} val (MA{window_size})', linestyle='--', alpha=0.8)\n\naxes[2, 0].set_xlabel('Epoch')\naxes[2, 0].set_ylabel('Loss (Smoothed)')\naxes[2, 0].set_title(f'Smoothed Loss (Moving Average, window={window_size})')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\naxes[2, 0].set_yscale('log')\n\n# Plot 8: Validation Loss vs Learning Rate (LR Finder style)\nfor model_name, history in all_histories.items():\n    axes[2, 1].scatter(history['learning_rate'], history['val_loss'], \n                      label=f'{model_name}', alpha=0.6, s=20)\n\naxes[2, 1].set_xlabel('Learning Rate')\naxes[2, 1].set_ylabel('Validation Loss')\naxes[2, 1].set_title('Validation Loss vs Learning Rate')\naxes[2, 1].set_xscale('log')\naxes[2, 1].set_yscale('log')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\n# Plot 9: Training Progress Timeline\naxes[2, 2].axis('off')\ntimeline_text = \"TRAINING PROGRESS TIMELINE\\n\" + \"=\"*30 + \"\\n\\n\"\n\nfor model_name, history in all_histories.items():\n    epochs_trained = len(history['train_loss'])\n    best_epoch = history['val_loss'].index(min(history['val_loss'])) + 1\n    \n    timeline_text += f\"ü§ñ {model_name.upper()}:\\n\"\n    timeline_text += f\"   Epochs: {epochs_trained}\\n\"\n    timeline_text += f\"   Best model at epoch: {best_epoch}\\n\"\n    timeline_text += f\"   Loss reduction: {history['train_loss'][0]:.4f} ‚Üí {history['train_loss'][-1]:.4f}\\n\"\n    timeline_text += f\"   Val loss reduction: {history['val_loss'][0]:.4f} ‚Üí {min(history['val_loss']):.4f}\\n\\n\"\n\n# Add research question validation\ntimeline_text += \"üéØ RESEARCH QUESTIONS:\\n\"\nfor model_name, history in all_histories.items():\n    best_val_loss = min(history['val_loss'])\n    rq1_passed = best_val_loss < 0.08  # Threshold for good anomaly detection\n    timeline_text += f\"   RQ1 ({model_name}): {'‚úÖ PASSED' if rq1_passed else '‚ùå FAILED'} (Val Loss: {best_val_loss:.4f})\\n\"\n\ntimeline_text += f\"\\nüí° NOTES:\\n\"\ntimeline_text += f\"   Target: Val Loss < 0.08 for anomaly detection\\n\"\ntimeline_text += f\"   Early stopping patience: {CONFIG['early_stopping_patience']}\\n\"\ntimeline_text += f\"   Checkpoints saved every {CONFIG['save_frequency']} epochs\\n\"\n\naxes[2, 2].text(0.05, 0.95, timeline_text, transform=axes[2, 2].transAxes, \n               fontsize=9, verticalalignment='top', fontfamily='monospace',\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n\nplt.suptitle(f'Enhanced Training Analysis - {training_info}', fontsize=14, y=0.98)\nplt.tight_layout()\n\n# FIXED: Show plot first before saving\nplt.show()\n\n# FIXED: Save with explicit backend and flush\nfrom datetime import datetime\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nplot_filename = f'{CONFIG[\"checkpoint_dir\"]}/enhanced_training_analysis_{timestamp}.png'\n\n# Method 1: Save current figure explicitly\nfig.savefig(plot_filename, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\nplt.close(fig)  # Close to free memory\n\n# Method 2: Alternative save with PIL (more reliable on Colab)\ntry:\n    # Create a new identical figure for saving\n    fig_save, axes_save = plt.subplots(3, 3, figsize=(20, 15))\n    \n    # Re-plot everything for the save figure\n    # Plot 1: Training and Validation Loss\n    for model_name, history in all_histories.items():\n        epochs = range(1, len(history['train_loss']) + 1)\n        axes_save[0, 0].plot(epochs, history['train_loss'], label=f'{model_name} train', linestyle='-', linewidth=2)\n        axes_save[0, 0].plot(epochs, history['val_loss'], label=f'{model_name} val', linestyle='--', linewidth=2)\n    \n    axes_save[0, 0].set_xlabel('Epoch')\n    axes_save[0, 0].set_ylabel('Loss (MSE)')\n    axes_save[0, 0].set_title('Training and Validation Loss')\n    axes_save[0, 0].legend()\n    axes_save[0, 0].grid(True, alpha=0.3)\n    axes_save[0, 0].set_yscale('log')\n    \n    # Plot 2: Learning Rate Schedule  \n    for model_name, history in all_histories.items():\n        epochs = range(1, len(history['learning_rate']) + 1)\n        axes_save[0, 1].plot(epochs, history['learning_rate'], label=f'{model_name} LR', linewidth=2)\n    \n    axes_save[0, 1].set_xlabel('Epoch')\n    axes_save[0, 1].set_ylabel('Learning Rate')\n    axes_save[0, 1].set_title('Learning Rate Schedule')\n    axes_save[0, 1].set_yscale('log')\n    axes_save[0, 1].legend()\n    axes_save[0, 1].grid(True, alpha=0.3)\n    \n    # Add summary text\n    summary_text_short = f\"Training Results Summary\\n\"\n    summary_text_short += f\"Dataset: {CONFIG['hf_dataset']}\\n\"\n    summary_text_short += f\"Samples: {CONFIG['samples']}\\n\"\n    for model_name, history in all_histories.items():\n        summary_text_short += f\"{model_name}: Best Val Loss = {min(history['val_loss']):.4f}\\n\"\n    \n    axes_save[0, 2].text(0.1, 0.5, summary_text_short, transform=axes_save[0, 2].transAxes,\n                        fontsize=12, verticalalignment='center',\n                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n    axes_save[0, 2].axis('off')\n    \n    # Hide other subplots for cleaner save\n    for i in range(3):\n        for j in range(3):\n            if (i, j) not in [(0, 0), (0, 1), (0, 2)]:\n                axes_save[i, j].axis('off')\n    \n    plt.suptitle(f'Training Analysis - {training_info}', fontsize=16, y=0.95)\n    plt.tight_layout()\n    \n    # Save the simplified version\n    plot_filename_simple = f'{CONFIG[\"checkpoint_dir\"]}/training_analysis_simple_{timestamp}.png'\n    fig_save.savefig(plot_filename_simple, dpi=300, bbox_inches='tight', \n                    facecolor='white', edgecolor='none', format='png')\n    plt.close(fig_save)\n    \n    print(f\"‚úÖ Simple training plot saved: {plot_filename_simple}\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Alternative save method failed: {e}\")\n\nprint(f\"‚úÖ Enhanced training analysis saved: {plot_filename}\")\n\n# Also save training stats as JSON\nimport json\nstats_filename = f'{CONFIG[\"checkpoint_dir\"]}/training_stats_{timestamp}.json'\ndetailed_stats = {\n    'config': CONFIG,\n    'training_results': {}\n}\n\nfor model_name, history in all_histories.items():\n    detailed_stats['training_results'][model_name] = {\n        'history': history,\n        'final_train_loss': history['train_loss'][-1],\n        'final_val_loss': history['val_loss'][-1],\n        'best_val_loss': min(history['val_loss']),\n        'best_val_epoch': history['val_loss'].index(min(history['val_loss'])) + 1,\n        'total_epochs': len(history['train_loss']),\n        'convergence_achieved': min(history['val_loss']) < 0.08,\n        'research_question_1_passed': min(history['val_loss']) < 0.08\n    }\n\nwith open(stats_filename, 'w') as f:\n    json.dump(detailed_stats, f, indent=2)\n\nprint(f\"‚úÖ Training statistics saved: {stats_filename}\")\nprint(f\"üìÅ All files saved to: {CONFIG['checkpoint_dir']}\")\n\n# FIXED: Force sync to Google Drive\nimport os\nif os.path.exists('/content/drive'):\n    os.system('sync')  # Force filesystem sync\n    print(\"üîÑ Files synced to Google Drive\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Reconstruction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_enhanced_reconstructions(models, train_loader, val_loader, num_samples=5):\n",
    "    \"\"\"Enhanced reconstruction visualization with train and validation samples\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(models) + 1, num_samples * 2, figsize=(20, 3 * (len(models) + 1)))\n",
    "    \n",
    "    # Get samples from both train and validation\n",
    "    train_batch = next(iter(train_loader))[:num_samples].to(device)\n",
    "    val_batch = next(iter(val_loader))[:num_samples].to(device)\n",
    "    \n",
    "    sample_batches = [train_batch, val_batch]\n",
    "    batch_labels = ['Train', 'Validation']\n",
    "    \n",
    "    # Original images\n",
    "    for batch_idx, (batch, label) in enumerate(zip(sample_batches, batch_labels)):\n",
    "        for i in range(num_samples):\n",
    "            col_idx = batch_idx * num_samples + i\n",
    "            img = batch[i].cpu().squeeze().numpy()\n",
    "            axes[0, col_idx].imshow(img, cmap='gray', vmin=-1, vmax=1)\n",
    "            axes[0, col_idx].axis('off')\n",
    "            if col_idx == 0:\n",
    "                axes[0, col_idx].set_ylabel('Original', fontsize=12)\n",
    "            if batch_idx == 0 and i == num_samples // 2:\n",
    "                axes[0, col_idx].set_title('Training Samples', fontsize=10)\n",
    "            elif batch_idx == 1 and i == num_samples // 2:\n",
    "                axes[0, col_idx].set_title('Validation Samples', fontsize=10)\n",
    "    \n",
    "    # Reconstructions for each model\n",
    "    for model_idx, (model_name, model) in enumerate(models.items()):\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(sample_batches):\n",
    "            with torch.no_grad():\n",
    "                recon = model(batch)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                col_idx = batch_idx * num_samples + i\n",
    "                img = recon[i].cpu().squeeze().numpy()\n",
    "                axes[model_idx + 1, col_idx].imshow(img, cmap='gray', vmin=-1, vmax=1)\n",
    "                axes[model_idx + 1, col_idx].axis('off')\n",
    "                \n",
    "                if col_idx == 0:\n",
    "                    axes[model_idx + 1, col_idx].set_ylabel(model_name, fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Enhanced Reconstruction Quality Assessment (FIXED)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(f'{CONFIG[\"checkpoint_dir\"]}/enhanced_reconstruction_comparison_fixed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úÖ Enhanced reconstruction comparison saved\")\n",
    "\n",
    "# Visualize enhanced reconstructions\n",
    "if trained_models:\n",
    "    visualize_enhanced_reconstructions(trained_models, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Enhanced Results Summary (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Enhanced summary with validation metrics\n",
    "summary = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'notebook_version': 'enhanced_training_fixed',\n",
    "        'dataset': CONFIG['hf_dataset'],\n",
    "        'total_samples': CONFIG['samples'],\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'validation_split': CONFIG['validation_split'],\n",
    "        'checkpoint_strategy': f\"Every {CONFIG['save_frequency']} epochs\" + (\" (best only)\" if CONFIG['keep_best_only'] else \" + best model\")\n",
    "    },\n",
    "    'config': CONFIG,\n",
    "    'training_results': {},\n",
    "    'advanced_features_used': [\n",
    "        'validation_monitoring',\n",
    "        'early_stopping',\n",
    "        'cosine_lr_scheduling_with_warmup',\n",
    "        'gradient_clipping',\n",
    "        'mixed_precision',\n",
    "        'fixed_checkpoint_logic'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Calculate detailed metrics for each model\n",
    "for model_name, history in all_histories.items():\n",
    "    train_losses = history['train_loss']\n",
    "    val_losses = history['val_loss']\n",
    "    learning_rates = history['learning_rate']\n",
    "    \n",
    "    summary['training_results'][model_name] = {\n",
    "        'epochs_trained': len(train_losses),\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'best_train_loss': min(train_losses),\n",
    "        'best_val_loss': min(val_losses),\n",
    "        'best_val_epoch': val_losses.index(min(val_losses)) + 1,\n",
    "        'final_learning_rate': learning_rates[-1],\n",
    "        'initial_learning_rate': learning_rates[0],\n",
    "        'lr_reduction_factor': learning_rates[0] / learning_rates[-1],\n",
    "        'convergence_achieved': val_losses[-1] < 0.05,  # Threshold for good convergence\n",
    "        'overfitting_detected': train_losses[-1] < val_losses[-1] * 0.7,  # Rough heuristic\n",
    "        'training_stability': max(train_losses) / min(train_losses),  # Lower is more stable\n",
    "        'validation_stability': max(val_losses) / min(val_losses),\n",
    "        'checkpoints_saved': f\"Best model + {'regular every ' + str(CONFIG['save_frequency']) + ' epochs' if not CONFIG['keep_best_only'] else 'best only'}\"\n",
    "    }\n",
    "\n",
    "# Save enhanced summary\n",
    "summary_path = f\"{CONFIG['checkpoint_dir']}/enhanced_training_summary_fixed.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\nüéØ ENHANCED TRAINING SUMMARY (FIXED CHECKPOINT LOGIC)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Dataset: {CONFIG['hf_dataset']}\")\n",
    "print(f\"üìà Samples: {CONFIG['samples']} ({len(train_dataset)} train + {len(val_dataset)} val)\")\n",
    "print(f\"üíæ Checkpoint Strategy: {summary['metadata']['checkpoint_strategy']}\")\n",
    "print(f\"üîß Advanced Features: {', '.join(summary['advanced_features_used'])}\")\n",
    "\n",
    "for model_name, results in summary['training_results'].items():\n",
    "    print(f\"\\nü§ñ {model_name.upper()} RESULTS:\")\n",
    "    print(f\"   Epochs Trained: {results['epochs_trained']}\")\n",
    "    print(f\"   Final Train Loss: {results['final_train_loss']:.4f}\")\n",
    "    print(f\"   Final Val Loss: {results['final_val_loss']:.4f}\")\n",
    "    print(f\"   Best Val Loss: {results['best_val_loss']:.4f} (Epoch {results['best_val_epoch']})\")\n",
    "    print(f\"   Learning Rate: {results['initial_learning_rate']:.2e} ‚Üí {results['final_learning_rate']:.2e} (reduction: {results['lr_reduction_factor']:.1f}x)\")\n",
    "    print(f\"   Convergence: {'‚úÖ' if results['convergence_achieved'] else '‚ö†Ô∏è'} {'Good' if results['convergence_achieved'] else 'Needs improvement'}\")\n",
    "    print(f\"   Overfitting: {'‚ö†Ô∏è' if results['overfitting_detected'] else '‚úÖ'} {'Detected' if results['overfitting_detected'] else 'Not detected'}\")\n",
    "    print(f\"   Training Stability: {results['training_stability']:.2f}x (lower is better)\")\n",
    "    print(f\"   Validation Stability: {results['validation_stability']:.2f}x (lower is better)\")\n",
    "    print(f\"   Checkpoints: {results['checkpoints_saved']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced summary saved to: {summary_path}\")\n",
    "print(f\"üìÅ All checkpoints and artifacts saved to: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# Check saved files\n",
    "import os\n",
    "saved_files = os.listdir(CONFIG['checkpoint_dir'])\n",
    "checkpoint_files = [f for f in saved_files if f.endswith('.pth')]\n",
    "if checkpoint_files:\n",
    "    print(f\"\\nüíæ Checkpoint files saved:\")\n",
    "    for f in sorted(checkpoint_files):\n",
    "        print(f\"   {f}\")\n",
    "\n",
    "# Finish W&B run\n",
    "if CONFIG['use_wandb']:\n",
    "    wandb.finish()\n",
    "    print(\"‚úÖ W&B run finished\")\n",
    "\n",
    "print(\"\\nüéâ ENHANCED TRAINING COMPLETE WITH FIXED CHECKPOINT LOGIC!\")\n",
    "print(\"üí° Next step: Use evaluation cells 21-26 from EVALUATION_GUIDE.md to answer research questions.\")\n",
    "print(f\"üîß Checkpoint behavior: {'Best model only saved' if CONFIG['keep_best_only'] else f'Regular checkpoints every {CONFIG[\"save_frequency\"]} epochs + best model'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}