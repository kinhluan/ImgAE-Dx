# ImgAE-Dx Poetry Architecture

## Modern Python Package Structure with Poetry

### üèóÔ∏è Core Architecture (Updated for Poetry)

```
imgae-dx/                        # Poetry project root
‚îú‚îÄ‚îÄ pyproject.toml              # Poetry configuration
‚îú‚îÄ‚îÄ README.md                   # Project documentation
‚îú‚îÄ‚îÄ CHANGELOG.md                # Version history
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ imgae_dx/               # Main package (importable)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Package initialization
‚îÇ       ‚îú‚îÄ‚îÄ cli/                # Command-line interface
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ train.py        # Training commands
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py     # Evaluation commands
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config.py       # Config management commands
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ notebook.py     # Notebook utilities
‚îÇ       ‚îú‚îÄ‚îÄ streaming/          # Cloud-native data streaming
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_client.py      # Kaggle API wrapper
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ stream_loader.py      # Streaming data loader
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.py     # Smart caching system
‚îÇ       ‚îú‚îÄ‚îÄ data/               # Data processing pipeline
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ streaming_dataset.py  # PyTorch Dataset with streaming
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ transforms.py         # Image preprocessing
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ batch_processor.py    # Memory-efficient batching
‚îÇ       ‚îú‚îÄ‚îÄ models/             # Model architectures
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ base.py         # Base model interface
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ unet.py         # U-Net architecture
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ reversed_ae.py  # Reversed Autoencoder
‚îÇ       ‚îú‚îÄ‚îÄ training/           # Training system
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py      # Base trainer class
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ streaming_trainer.py # Cloud-native trainer
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py    # Model evaluation
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py      # Custom metrics (AUC, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ utils/              # Utilities and helpers
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config_manager.py     # Configuration management (existing)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py # Model checkpointing
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ logging_utils.py      # Logging and W&B integration
‚îÇ       ‚îî‚îÄ‚îÄ visualization/      # Plotting and visualization
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ plots.py        # Standard plotting utilities
‚îÇ           ‚îú‚îÄ‚îÄ streaming_plots.py    # Real-time visualization
‚îÇ           ‚îî‚îÄ‚îÄ medical_viz.py  # Medical imaging specific plots
‚îÇ
‚îú‚îÄ‚îÄ configs/                    # Configuration files (existing)
‚îÇ   ‚îú‚îÄ‚îÄ project_config.yaml     # Main project configuration
‚îÇ   ‚îú‚îÄ‚îÄ kaggle.json            # Kaggle API credentials
‚îÇ   ‚îú‚îÄ‚îÄ wandb.json             # W&B API credentials  
‚îÇ   ‚îî‚îÄ‚îÄ model_configs/         # Model-specific configurations
‚îÇ       ‚îú‚îÄ‚îÄ unet.yaml
‚îÇ       ‚îî‚îÄ‚îÄ reversed_ae.yaml
‚îÇ
‚îú‚îÄ‚îÄ scripts/                   # Executable scripts
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ train_streaming.py     # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_models.py     # Model evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ setup_environment.py   # Environment setup
‚îÇ   ‚îî‚îÄ‚îÄ generate_notebooks.py  # Auto-generate research notebooks
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                 # Research notebooks (existing)
‚îÇ   ‚îú‚îÄ‚îÄ research/              # Original research notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Anomaly_Detection_Research.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Anomaly_Detection_Research_Colab.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ examples/             # Example notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quick_start.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ advanced_usage.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ generated/            # Auto-generated notebooks
‚îÇ       ‚îî‚îÄ‚îÄ streaming_demo.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/                    # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py          # Pytest configuration
‚îÇ   ‚îú‚îÄ‚îÄ unit/                # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_data.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/         # Integration tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_streaming.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_training.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/           # Test data and fixtures
‚îÇ       ‚îú‚îÄ‚îÄ sample_images/
‚îÇ       ‚îî‚îÄ‚îÄ test_configs/
‚îÇ
‚îú‚îÄ‚îÄ docs/                   # Project documentation (existing)
‚îÇ   ‚îú‚îÄ‚îÄ api/               # Auto-generated API docs
‚îÇ   ‚îú‚îÄ‚îÄ tutorials/         # User tutorials
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPMENT_TODO.md # Development roadmap
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_JOURNEY.md  # Research methodology
‚îÇ   ‚îî‚îÄ‚îÄ ...               # Other existing docs
‚îÇ
‚îî‚îÄ‚îÄ .github/              # GitHub workflows (optional)
    ‚îî‚îÄ‚îÄ workflows/
        ‚îú‚îÄ‚îÄ test.yml      # CI/CD testing
        ‚îú‚îÄ‚îÄ publish.yml   # Package publishing
        ‚îî‚îÄ‚îÄ docs.yml      # Documentation building
```

---

## üîÑ Streaming Data Flow

### 1. **Kaggle API Integration**

```python
class KaggleStreamClient:
    def __init__(self, dataset="nih-chest-xray/data"):
        self.api = kaggle.api
        self.dataset = dataset
        
    def stream_zip_content(self, filename="images_001.zip"):
        """Stream ZIP content directly to memory"""
        # Download to temporary buffer
        temp_buffer = self.api.dataset_download_file(
            self.dataset, filename, 
            path="memory://", unzip=False
        )
        
        # Return ZipFile object for streaming access
        return zipfile.ZipFile(io.BytesIO(temp_buffer))
        
    def get_progressive_streams(self):
        """Get multiple stage streams progressively"""
        stages = ["images_001.zip", "images_002.zip", "images_003.zip"]
        for stage in stages:
            yield self.stream_zip_content(stage)
```

### 2. **Streaming Dataset Implementation**

```python
class StreamingNIHDataset(Dataset):
    def __init__(self, kaggle_client, stage="images_001", transform=None):
        self.kaggle_client = kaggle_client
        self.stage = stage
        self.transform = transform
        
        # Stream metadata first
        self.metadata = self._load_metadata_streaming()
        self.image_list = self._build_image_index()
        
        # Initialize streaming ZIP
        self.current_zip = None
        self.zip_cache = {}  # Smart caching for frequently accessed images
        
    def __getitem__(self, idx):
        img_info = self.image_list[idx]
        
        # Get image from streaming source
        image = self._get_image_streaming(img_info['filename'])
        label = img_info['label']
        
        if self.transform:
            image = self.transform(image)
            
        return image, label
        
    def _get_image_streaming(self, filename):
        """Stream individual image from Kaggle ZIP"""
        if self.current_zip is None:
            self.current_zip = self.kaggle_client.stream_zip_content(
                f"{self.stage}.zip"
            )
            
        # Extract image from ZIP stream
        with self.current_zip.open(filename) as img_file:
            image = Image.open(img_file).convert('L')
            
        return image
```

### 3. **Memory-Efficient Training Loop**

```python
class StreamingTrainer:
    def __init__(self, model, kaggle_client, config):
        self.model = model
        self.kaggle_client = kaggle_client
        self.config = config
        self.memory_manager = StreamingMemoryManager()
        
    def train_progressive_stages(self):
        stages = ["images_001", "images_002", "images_003"]
        
        for stage_idx, stage in enumerate(stages):
            print(f"Training Stage {stage_idx + 1}: {stage}")
            
            # Create streaming dataset for current stage
            dataset = StreamingNIHDataset(
                self.kaggle_client, 
                stage=stage,
                transform=self.get_transforms()
            )
            
            dataloader = DataLoader(
                dataset, 
                batch_size=self.config.batch_size,
                num_workers=0  # Single process for streaming
            )
            
            # Train with current stage
            stage_metrics = self.train_single_stage(
                dataloader, 
                stage_name=stage,
                resume_from=f"stage_{stage_idx}.pth" if stage_idx > 0 else None
            )
            
            # Save stage checkpoint to cloud
            self.save_cloud_checkpoint(f"stage_{stage_idx + 1}.pth", stage_metrics)
            
            # Memory cleanup
            self.memory_manager.cleanup_stage(stage)
            
    def train_single_stage(self, dataloader, stage_name, resume_from=None):
        if resume_from and self.cloud_checkpoint_exists(resume_from):
            self.load_cloud_checkpoint(resume_from)
            
        for epoch in range(self.config.epochs_per_stage):
            for batch_idx, (images, labels) in enumerate(dataloader):
                # Standard training loop
                loss = self.train_batch(images, labels)
                
                # Real-time logging to W&B
                self.log_streaming_metrics(loss, batch_idx, stage_name)
                
                # Memory management
                if batch_idx % 100 == 0:
                    self.memory_manager.check_memory_usage()
                    
        return {"stage": stage_name, "final_loss": loss}
```

---

## ‚òÅÔ∏è Cloud-Native Features

### 1. **Google Colab Integration**

```python
class ColabStreamingSetup:
    def __init__(self):
        self.setup_kaggle_auth()
        self.setup_drive_integration()
        self.setup_gpu_optimization()
        
    def setup_kaggle_auth(self):
        """Setup Kaggle API in Colab"""
        from google.colab import files
        
        # Auto-upload kaggle.json if not exists
        if not os.path.exists('/root/.kaggle/kaggle.json'):
            print("Please upload your kaggle.json file:")
            uploaded = files.upload()
            
        # Configure permissions
        os.chmod('/root/.kaggle/kaggle.json', 600)
        
    def setup_drive_integration(self):
        """Mount Google Drive for checkpoints"""
        from google.colab import drive
        drive.mount('/content/drive')
        
        # Create checkpoint directory
        self.checkpoint_dir = '/content/drive/MyDrive/ImgAE-Dx-Checkpoints'
        os.makedirs(self.checkpoint_dir, exist_ok=True)
```

### 2. **Real-Time Visualization**

```python
class StreamingVisualizer:
    def __init__(self, wandb_project="imgae-dx-streaming"):
        import wandb
        self.wandb = wandb
        self.wandb.init(project=wandb_project)
        
    def log_streaming_progress(self, metrics, stage, batch_idx):
        """Log real-time training progress"""
        self.wandb.log({
            f"{stage}/loss": metrics['loss'],
            f"{stage}/batch": batch_idx,
            "memory_usage": self.get_memory_usage(),
            "streaming_speed": metrics.get('samples_per_sec', 0)
        })
        
    def visualize_streaming_comparison(self, unet_metrics, ra_metrics):
        """Real-time model comparison"""
        comparison_table = self.wandb.Table(
            columns=["Stage", "Model", "AUC", "Loss"],
            data=[
                ["Current", "U-Net", unet_metrics['auc'], unet_metrics['loss']],
                ["Current", "RA", ra_metrics['auc'], ra_metrics['loss']]
            ]
        )
        self.wandb.log({"model_comparison": comparison_table})
```

### 3. **Automatic Notebook Generation**

```python
class StreamingNotebookGenerator:
    def generate_colab_notebook(self):
        """Generate research-ready Colab notebook"""
        notebook = {
            "cells": [
                self.create_setup_cell(),
                self.create_streaming_demo_cell(),
                self.create_training_cell(),
                self.create_evaluation_cell(),
                self.create_visualization_cell()
            ]
        }
        
        # Save to notebooks/generated/
        with open("notebooks/generated/streaming_research.ipynb", "w") as f:
            json.dump(notebook, f, indent=2)
            
    def create_streaming_demo_cell(self):
        return {
            "cell_type": "code",
            "source": [
                "# Streaming Demo: Direct Kaggle Access",
                "kaggle_client = KaggleStreamClient('nih-chest-xray/data')",
                "",
                "# Stream images_001 directly",
                "dataset = StreamingNIHDataset(kaggle_client, stage='images_001')",
                "print(f'Streaming {len(dataset)} images from Kaggle')",
                "",
                "# Show streaming progress",
                "for i in tqdm(range(0, 100, 10)):",
                "    img, label = dataset[i]",
                "    print(f'Streamed image {i}: shape={img.shape}, label={label}')"
            ]
        }
```

---

## üöÄ Poetry Usage Examples

### Development Workflow

```bash
# Install project with Poetry
poetry install --with dev,cloud,viz

# Activate virtual environment
poetry shell

# CLI commands through Poetry
poetry run imgae-train --config configs/project_config.yaml --stage images_001
poetry run imgae-evaluate --model checkpoints/unet_best.pth
poetry run imgae-config --validate-apis

# Run tests
poetry run pytest tests/ -v --cov

# Code quality checks
poetry run black src/ tests/
poetry run isort src/ tests/
poetry run mypy src/

# Build and publish
poetry build
poetry publish --repository pypi
```

### Google Colab with Poetry

```python
# Install from PyPI (after publishing)
!pip install imgae-dx[cloud]

# Or install from GitHub (development)
!pip install git+https://github.com/user/imgae-dx.git

from imgae_dx.streaming import ColabStreamingSetup, StreamingTrainer
from imgae_dx.cli import setup_colab_environment

# Auto-setup Colab environment
setup_colab_environment()

# Stream and train directly  
trainer = StreamingTrainer.from_config("configs/project_config.yaml")
results = trainer.train_progressive_stages()

print(f"Training complete! AUC scores: {results}")
```

### Package Import Structure

```python
# Clean imports with Poetry structure
from imgae_dx.models import UNet, ReversedAutoencoder
from imgae_dx.data import StreamingNIHDataset
from imgae_dx.training import StreamingTrainer, Evaluator
from imgae_dx.utils import ConfigManager
from imgae_dx.visualization import plot_roc_curve, visualize_anomalies

# Or use factory patterns
from imgae_dx import create_model, create_trainer, load_config

config = load_config("configs/project_config.yaml")
model = create_model("unet", config.model)
trainer = create_trainer("streaming", model, config.training)
```

---

## üìä Poetry Architecture Benefits

### Package Management

- **Dependency Resolution**: Automatic conflict resolution with lock file
- **Virtual Environments**: Isolated environments per project
- **Version Pinning**: Reproducible builds across environments
- **Optional Dependencies**: Flexible installation (cloud, viz, performance groups)

### Development Experience

- **CLI Integration**: Built-in commands via `poetry run imgae-train`
- **Testing Framework**: Integrated pytest with coverage
- **Code Quality**: Black, isort, mypy integration
- **Documentation**: Sphinx auto-generation from docstrings

### Distribution & Publishing

- **PyPI Ready**: One-command publishing to package indexes
- **GitHub Integration**: Direct pip install from repository
- **Semantic Versioning**: Automated version management
- **Cross-Platform**: Works on Windows, macOS, Linux

### Cloud Integration

- **Colab Compatibility**: Easy pip install in notebooks
- **Memory Efficiency**: ~500MB peak memory usage with streaming
- **GPU Optimization**: Continuous GPU utilization
- **Checkpoint Management**: Auto-sync with Google Drive

### Professional Development

- **CI/CD Ready**: GitHub Actions integration
- **Type Safety**: Full mypy type checking
- **Test Coverage**: Automated coverage reporting  
- **Documentation**: Auto-generated API docs

This modern Poetry architecture provides enterprise-grade package management while maintaining the streaming performance and cloud-native features!
